# Joe Alcantara (Jomar Alcantara)

**AI Safety Researcher | Teaching Fellow at University of Warwick**

## About

I'm an AI Safety Researcher investigating safer approaches to AI development through interpretable causal world models and few-shot catastrophe prevention. I also teach Computer Science foundations at the University of Warwick.

## Research Interests

### 1. Scientist AI: Interpretable World Models Without Agency

**Research question:** Can we build safer, more interpretable AI through passive causal world model learning?

I'm exploring whether removing agency (goal-seeking behavior) while retaining learning capability can lead to fundamentally safer AI systems. My approach involves implementing Scientist AI at scale to demonstrate that interpretable causal world models are both practical and safer than black-box optimization.

**Current work:** A 4-paper research arc covering implementation, scalability, safety properties, and application of Scientist AI as an alternative to traditional RL agents.

### 2. Few-Shot Catastrophe Prevention

**Research question:** Can we detect and prevent AI failures using learned causal structures from limited examples?

When we catch AI misbehavior, what's the best way to use that information to prevent future failures? I'm investigating how causal world models can identify dangerous patterns from few examples of AI misbehavior, creating practical tools for AI labs to analyze caught failures and predict similar risks before they occur.

### 3. Infrastructure Sovereignty & Digital Vassalage

**Research question:** How do nations without economic leverage negotiate fair terms for AI infrastructure in an oligopolistic market?

Middle-power nations like the Philippines get worse deals for compute and AI services compared to how the UK negotiates - the global AI infrastructure market replicates existing power hierarchies. I'm systematically analyzing the **terms** of AI infrastructure dependency to identify strategic options for uncompetitive economies. This connects to AI safety: global AI safety requires infrastructure equity. Nations without good infrastructure terms can't build domestic AI safety capacity.

**Key angle:** The Philippines' 1.3M-person BPO industry faces an AI displacement crisis while simultaneously lacking infrastructure leverage to shape AI adoption terms - creating a dangerous dependency trap.

## Current Phase

**Research:** Foundations and Scientist AI implementation (Paper 1)
**Teaching:** Computer Science foundations at University of Warwick

## Contact

- Email: jomar.alcantara@gmail.com
- Website: [joealcantara.com](https://joealcantara.com)
- Twitter/X: [@joealcantara_](https://x.com/joealcantara_)
- Location: UK

---

*Building safer AI systems through interpretable world models.*
